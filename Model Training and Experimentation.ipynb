{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import re\n",
    "\n",
    "#Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "#spacy\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "#vis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "#nltk.download(punkt)\n",
    "#nltk.download(wordnet) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##\"local_data.json\" deleted from this repo for to preserve confidentiality of narratives used during training\n",
    "def load_data(file):\n",
    "    with open (file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f) \n",
    "    return (data)\n",
    "\n",
    "def write_data(file, data):\n",
    "    with open (file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "stories = load_data(\"local_data.json\")[\"File\"]\n",
    "print(stories[0][:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process and vectorize the documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "MFyKGQ-x3IZV"
   },
   "outputs": [],
   "source": [
    "#In case prompted to install stopwords, use \"nltk.download(stopwords)\".\n",
    "#Lemmatization\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    #Use \"python -m spacy download en_core_web_sm\" if error [E050]\n",
    "    nlp = spacy.load(\"en_core_web_sm\",disable=[\"parser\",\"ner\"])\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        new_text = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags:\n",
    "                new_text.append(token.lemma_)\n",
    "        final = \" \".join(new_text)\n",
    "        texts_out.append(final)\n",
    "        \n",
    "    return(texts_out)\n",
    "\n",
    "lemmatized_texts = lemmatization(stories)\n",
    "print(lemmatized_texts[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deaccenting and removing stop words\n",
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return (final)\n",
    "\n",
    "data_words = gen_words(lemmatized_texts)\n",
    "print (data_words[0][0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#BIGRAMS AND TRIGRAMS\n",
    "bigram_phrases = gensim.models.Phrases(data_words, min_count=2,threshold=50)\n",
    "trigram_phrases = gensim.models.Phrases(bigram_phrases[data_words],threshold=50)\n",
    "\n",
    "bigram = gensim.models.phrases.Phraser(bigram_phrases)\n",
    "trigram = gensim.models.phrases.Phraser(trigram_phrases)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return(bigram[doc] for doc in texts)\n",
    "\n",
    "def make_trigrams(texts): \n",
    "    return(trigram[bigram[doc]] for doc in texts)\n",
    "\n",
    "data_bigrams = make_bigrams(data_words)\n",
    "data_bigrams_trigrams = make_trigrams(data_bigrams)\n",
    "\n",
    "data_bigrams_trigrams = list(data_bigrams_trigrams)\n",
    "print(data_bigrams_trigrams[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TF-IDF REMOVAL\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "id2word = corpora.Dictionary(data_bigrams_trigrams)\n",
    "\n",
    "texts = data_bigrams_trigrams\n",
    "print(texts[0][:20])\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "print (corpus[0][0:20])\n",
    "\n",
    "tfidf = TfidfModel(corpus, id2word=id2word)\n",
    "\n",
    "low_value = 0.03\n",
    "words  = []\n",
    "words_missing_in_tfidf = []\n",
    "for i in range(0, len(corpus)):\n",
    "    bow = corpus[i]\n",
    "    low_value_words = [] #reinitialize to be safe. You can skip this.\n",
    "    tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "    drops = low_value_words+words_missing_in_tfidf\n",
    "    for item in drops:\n",
    "        words.append(id2word[item])\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # The words with tf-idf socre 0 will be missing\n",
    "\n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "    corpus[i] = new_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Initial Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Edw7Y3MIJnLK"
   },
   "outputs": [],
   "source": [
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = 1  # Dont evaluate model perplexity, takes too much time\n",
    "\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus, \n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha=\"auto\",\n",
    "    eta=\"auto\",\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda_model.save(\"Models/test_model_1.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top topics detected by model (visualize for clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_topics = lda_model.top_topics(corpus)\n",
    "\n",
    "# # Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "# avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "# print(f'Average topic coherence: %.4f. % avg_topic_coherence')\n",
    "\n",
    "# from pprint import pprint\n",
    "# print(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cskTzFfzR-Kp"
   },
   "outputs": [],
   "source": [
    "new_model = gensim.models.ldamodel.LdaModel.load(\"Models/test_model_1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Corpus has bee taken out here, modify \"Data Input\" with \n",
    "new data to evaluate model.\n",
    "\"\"\"\n",
    "\n",
    "#testing the loaded model\n",
    "test_doc = corpus[-1]\n",
    "\n",
    "vector = new_model[test_doc]\n",
    "\n",
    "#logfile = open('Evaluation/For Model V1/Story_1.txt', 'a')\n",
    "# print(new_model.show_topics(num_topics=10, num_words=10, formatted=False), file = logfile)\n",
    "# logfile.close()\n",
    "\n",
    "#Sort to arrange topics based on priority or similarity. \n",
    "def Sort(sub_li):\n",
    "    sub_li.sort(key = lambda x: x[1])\n",
    "    sub_li.reverse()\n",
    "    return(sub_li)\n",
    "new_vector = Sort(vector)\n",
    "print(new_vector)#last story is most similar with topic 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 951
    },
    "collapsed": true,
    "id": "vhwBJnIhT9VZ",
    "outputId": "607d7393-24e0-42c4-ba97-9a7d0bbf94e1"
   },
   "outputs": [],
   "source": [
    "#use \"pip install pyLDAvis\" if the pyLDAvis library isn't installed on your system\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(new_model, corpus, id2word, mds=\"mmds\", R=30)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Coherence Score \n",
    "coherence_model_lda = CoherenceModel(\n",
    "    model = new_model,\n",
    "    texts = texts, \n",
    "    dictionary = id2word, \n",
    "    coherence = 'c_v'\n",
    ")\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a function to find the most optimal number of topics for a better coherence score. \n",
    "\"\"\"Source_1 = https://neptune.ai/blog/pyldavis-topic-modelling-exploration-tool-that-every-nlp-data-scientist-should-know\"\"\"\n",
    "\"\"\"Source_2 = https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#16buildingldamalletmodel\"\"\"\n",
    "\n",
    "def compute_coherence_values(chunksize, passes, iterations, eval_every, dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    " \n",
    " -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LdaModel(\n",
    "        corpus=corpus, \n",
    "        id2word=id2word,\n",
    "        chunksize=chunksize,\n",
    "        alpha=\"auto\",\n",
    "        eta=\"auto\",\n",
    "        iterations=iterations,\n",
    "        num_topics=num_topics,\n",
    "        passes=passes,\n",
    "        eval_every=eval_every\n",
    "        )\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(\n",
    "    dictionary=id2word, \n",
    "    corpus=corpus, \n",
    "    texts=texts, \n",
    "    start=2, \n",
    "    limit=40, \n",
    "    step=4, \n",
    "    chunksize = 2000, \n",
    "    passes = 20, \n",
    "    iterations = 400,\n",
    "    eval_every = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show graph\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "\n",
    "limit=40; start=2; step=4;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Extracting the topic that gives the best results. \n",
    "best_result_model = coherence_values.index(max(coherence_values))\n",
    "optimal_model = model_list[best_result_model]\n",
    "print(f'''{x[best_result_model]} topics give the highest coherence score of {coherence_values[best_result_model]} for this specific model''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(optimal_model, corpus, id2word, mds=\"mmds\", R=30)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal_model.save(\"Models/Model V2/test_model_2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = optimal_model[corpus]\n",
    "corpus_topics = [sorted(topics, key=lambda record: -record[1])[0] for topics in results]\n",
    "print(corpus_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [[(term, round(wt, 3)) for term, wt in optimal_model.show_topic(n, topn=20)] for n in range(0, optimal_model.num_topics)]\n",
    "# set column width\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "topics_df = pd.DataFrame([', '.join([term for term, wt in topic]) for topic in topics], columns = ['Terms per Topic'], index=['Topic'+str(t) for t in range(1, optimal_model.num_topics+1)] )\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Demo V1.1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
